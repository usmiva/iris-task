{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab766087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports to fine-tune BERT-like text classification model using transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this datset and the scientific domain, there is a specialized BERT model fine-tuned for scientific texts\n",
    "# and presented by Beltagy et al. - SCIBERT: A Pretrained Language Model for Scientific Text - https://aclanthology.org/D19-1371.pdf\n",
    "# model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Uncomment to use this model instead of distilBERT. Unsufficient RAM and CPU resources on the current setup for this model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679c6dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_datasets = load_dataset('csv', data_files={\n",
    "    'train': 'arxiv_200_multiclass_train.csv',\n",
    "    'validation': 'arxiv_200_multiclass_val.csv',\n",
    "    'test': 'arxiv_200_multiclass_test.csv'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dece13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd543764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e24b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = hf_datasets['train'].map(preprocess_function, batched=True)\n",
    "tokenized_val = hf_datasets['validation'].map(preprocess_function, batched=True)\n",
    "tokenized_test = hf_datasets['test'].map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420adb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_labels = ['hep-ph',\n",
    " 'physics',\n",
    " 'math',\n",
    " 'cond-mat',\n",
    " 'gr-qc',\n",
    " 'astro-ph',\n",
    " 'hep-th',\n",
    " 'hep-ex',\n",
    " 'nlin',\n",
    " 'q-bio',\n",
    " 'cs',\n",
    " 'nucl-th',\n",
    " 'quant-ph',\n",
    " 'nucl-ex',\n",
    " 'hep-lat',\n",
    " 'stat',\n",
    " 'q-fin',\n",
    " 'eess',\n",
    " 'econ']\n",
    "id2label = {i: label for i, label in enumerate(list_of_labels)}\n",
    "label2id = {label: i for i, label in enumerate(list_of_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093fed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the base model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=len(list_of_labels), label2id=label2id, id2label=id2label)\n",
    "\n",
    "# TODO: Test SciBERT model as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72facc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_multiclass\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    #compute_metrics=compute_metrics, This is not working here. Tried to change transformers versions but no success. Will compute them separately after training in an evalution notebook on the test set.\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    use_cpu=True,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fc2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    #tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1c98b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
